卷积神经网络(Convolutional Neural Network, CNN)
循环神经网络(Recurrent Neural Networks, RNN)
LSTM（Long Short-Term Memory）
词向量（word embedding）
    基于神经网络的分布表示（distributed representation）一般称为词向量

Tensor就是张量，张量可以被简单地理解为多维数组（n阶张量为n维数组）。
Flow直接翻译就是“流”，它直观地表达了张量之间通过计算相互转化的过程。
TensorFlow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。

全连接神经网络:相邻两层之间任意两个节点之间都有连接。
前向传播(forward propagation)
反向传播(Back propagation, BP)
    BP算法(即误差反向传播算法)是在有导师指导下，适合于多层神经元网络的一种学习算法，它建立在梯度下降（gradient decent）法
的基础上。
    梯度下降算法主要用于单个参数的取值，而反向传播算法给出了一个高效的方式在所有参数上使用梯度下降算法，从而试神经网络模型
在训练数据上的损失函数尽可能小。
    反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。
    BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。
如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏
导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。
    
TensorFLow中三个基本概念：计算图(tf.Graph)、张量(tf.Tensor)和会话(tf.Session)
    计算图是TensorFlow的计算模型，所有的TensorFlow的程序都会通过计算图的形式表示。计算图上的每个节点都是一个运算，而计算
图上的边则表示了运算之间的数据传递关系。
    TensorFlow中所有运算的输入、输出都是张量。张量本身并不储存任何数据，它只是对运算结果的引用。
    会话是TensorFlow的运算模型，它管理了一个TensorFlow程序拥有的系统资源，所有的运算都要通过会话执行。

维基百科对深度学习的定义为“一类通过多层非线性变换对高复杂性数据建模算法的合集”。深度学习两个重要特性：多层、非线性。

激活函数实现去线性化：如果将每一个神经元（也就是神经网络中的节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再
是线性的了。这个非线性函数就是激活函数。
