卷积神经网络(Convolutional Neural Network, CNN)
循环神经网络(Recurrent Neural Networks, RNN)
LSTM（Long Short-Term Memory）
词向量（word embedding）
    基于神经网络的分布表示（distributed representation）一般称为词向量

Tensor就是张量，张量可以被简单地理解为多维数组（n阶张量为n维数组）。
Flow直接翻译就是“流”，它直观地表达了张量之间通过计算相互转化的过程。
TensorFlow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。

全连接神经网络:相邻两层之间任意两个节点之间都有连接。
前向传播(forward propagation)
反向传播(Back propagation, BP)
    BP算法(即误差反向传播算法)是在有导师指导下，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。
    反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。
    BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。
如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏
导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。
    
TensorFLow中三个基本概念：计算图(tf.Graph)、张量(tf.Tensor)和会话(tf.Session)
    计算图是TensorFlow的计算模型，所有的TensorFlow的程序都会通过计算图的形式表示。计算图上的每个节点都是一个运算，而计算图
上的边则表示了运算之间的数据传递关系。
    TensorFlow中所有运算的输入、输出都是张量。张量本身并不储存任何数据，它只是对运算结果的引用。
    会话是TensorFlow的运算模型，它管理了一个TensorFlow程序拥有的系统资源，所有的运算都要通过会话执行。
    
